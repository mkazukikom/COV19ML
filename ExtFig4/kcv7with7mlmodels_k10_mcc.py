# -*- coding: utf-8 -*-
"""kCV7with7MLmodels_k10_mcc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WYHNidl_gAam-VmsLdH7jc75PJOt47Da

#1 環境設定

## 1) ランタイムに接続
"""

from google.colab import drive
drive.mount('/content/drive')

"""## 2) Optuna と XGBoost と CatBoost のインストール"""

# Optunaのインストール
!pip install optuna -q

# XGBoostのインストール
!pip3 install xgboost -q
!pip3 install -q pydot
!pip3 install graphviz -q

# CatBoostのインストール
!pip install catboost --quiet

"""# 2. 機械学習での評価

## 1) 入力先と出力先
"""

# 分析対象のExcelファイルのパスを path_in とする。
path_in = '/content/drive/MyDrive/COV19ML/10foldCV/combined_data.csv'
# 分析結果をExcelファイルで出力するフォルダーを path_out で指定する。
path_out = '/content/drive/MyDrive/COV19ML/10foldCV/'
# 予測モデルをPickleで保存するフォルダーを path_model で指定する。
path_model = '/content/drive/MyDrive/COV19ML/10foldCV/'

"""## 2) 計算: ExcelファイルかCSVファイルかに応じて、17行目を read_excel または read_csv とする"""

# Commented out IPython magic to ensure Python compatibility.
print('')
print('■ The k-fold Cross Validation with different Machine Learning models')
print('')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import pickle
import os
from collections import defaultdict
import warnings

from sklearn.model_selection import KFold, cross_val_score, cross_validate
from sklearn.metrics import (
    roc_auc_score,
    r2_score,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    matthews_corrcoef
)

warnings.simplefilter(action='ignore', category=UserWarning)

# データの読み込み。 path_inは分析対象のCSVファイルのGoogle Drive上のディレクトリ
df = pd.read_csv(str(path_in))

# データの大きさ
print('◆ 行と列の数')
print(df.shape)

# kの設定
n_samples = len(df)
print(f'The sample size of dataset: {n_samples}')

min_test_samples = 50
k = max(2, np.floor(n_samples / min_test_samples).astype(int))
max_k = 10
k = min(k, max_k)
# k-foldのkをマニュアルで決める場合は次の行を加える
k = 10

print(f"Chosen value of k for k-fold cross-validation: {k}")
print('')

# 定数の設定
random_state = 42
n_trialsA = 100
n_trialsB = 50
n_trialsC = 10

# Numpyのデータの作成
x = df.iloc[:, :-1].values
t = df.iloc[:, -1].values

# 目的変数の分布の表示
print('◆ 目的変数の分布')
sns.displot(df.iloc[:,-1].dropna())
print('陽性症例の割合(事前確率): ' + str(round(100*(np.count_nonzero(t>0)/len(t)), 5)) + ' %')
print('')
plt.show()

# 出力ファイル
from datetime import datetime
current_time = datetime.now()
formatted_time = current_time.strftime('%Y%m%d%H%M%S')

filename0 = os.path.basename(path_in)
filename = os.path.splitext(filename0)[0]
filepath1 = path_out + '/ResultsOf' + filename + 'withoutNB' + str(formatted_time) + '.xlsx'
filepath2 = path_out + '/ResultsOf' + filename + 'withNB' + str(formatted_time) + '.xlsx'
filepath3 = path_out + '/ResultsOf' + filename + 'wihtoutkNN&NB' + str(formatted_time) + '.xlsx'

print('◆ 計算結果の出力先')
print('1. Naive Bayes 以外の結果')
print(filepath1)
print('2. Naive Bayes も含めた結果')
print(filepath2)
print('3. Naive Bayes も k-Nearest Neighbours も含めない結果')
print(filepath3)
print('')
print('')

# MCCを計算する関数
def compute_mcc(y_true, y_pred):
    return matthews_corrcoef(y_true, y_pred)

###############################################################################
# Helper: CV結果をまとめて DataFrame を作る関数
#  (各モデルの関数で共通利用: metricsリストを受け取って mean/std を計算しDataFrame返却)
###############################################################################
def make_result_df(metrics, model_name):
    """
    metrics: dict 形式。たとえば
        {
            'auc': [fold1, fold2, ..., foldK],
            'acc': [...],
            'prec': [...],
            'rec': [...],
            'f1': [...],
            'mcc': [...]
        }
    model_name: str（DataFrameのindex用）
    """
    # 各指標の mean, std を計算
    auc_mean, auc_std = np.mean(metrics['auc']), np.std(metrics['auc'])
    acc_mean, acc_std = np.mean(metrics['acc']), np.std(metrics['acc'])
    prec_mean, prec_std = np.mean(metrics['prec']), np.std(metrics['prec'])
    rec_mean, rec_std = np.mean(metrics['rec']), np.std(metrics['rec'])
    f1_mean, f1_std = np.mean(metrics['f1']), np.std(metrics['f1'])
    mcc_mean, mcc_std = np.mean(metrics['mcc']), np.std(metrics['mcc'])

    # 表示
    print(f'■ {model_name}')
    print('')
    print(f'Mean AUC: {auc_mean:.4f} ± {auc_std:.4f}')
    print(f'Mean Accuracy: {acc_mean:.4f} ± {acc_std:.4f}')
    print(f'Mean Precision: {prec_mean:.4f} ± {prec_std:.4f}')
    print(f'Mean Recall: {rec_mean:.4f} ± {rec_std:.4f}')
    print(f'Mean F1 Score: {f1_mean:.4f} ± {f1_std:.4f}')
    print(f'Mean MCC: {mcc_mean:.4f} ± {mcc_std:.4f}')
    print('')

    # DataFrameにして返却
    df_result = pd.DataFrame(
        [[
            auc_mean, auc_std,
            acc_mean, acc_std,
            prec_mean, prec_std,
            rec_mean, rec_std,
            f1_mean, f1_std,
            mcc_mean, mcc_std
        ]],
        columns=[
            'AUC_mean','AUC_std',
            'Accuracy_mean','Accuracy_std',
            'Precision_mean','Precision_std',
            'Recall_mean','Recall_std',
            'f1-score_mean','f1-score_std',
            'MCC_mean','MCC_std'
        ],
        index=[model_name]
    )
    return df_result

##########################################
# simple Linear Regression
##########################################
def LinearKFold(k=k):
    from sklearn.linear_model import LinearRegression

    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    lr = LinearRegression()

    # 指標を格納するリスト
    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        lr.fit(X_train, y_train)
        predictions = lr.predict(X_test)

        # 0.5を閾値とした2値化
        predictions_round = [1 if p >= 0.5 else 0 for p in predictions]

        auc = roc_auc_score(y_test, predictions)
        acc = accuracy_score(y_test, predictions_round)
        prec = precision_score(y_test, predictions_round, zero_division=0)
        rec = recall_score(y_test, predictions_round)
        f1 = f1_score(y_test, predictions_round)
        mcc = compute_mcc(y_test, predictions_round)

        metrics['auc'].append(auc)
        metrics['acc'].append(acc)
        metrics['prec'].append(prec)
        metrics['rec'].append(rec)
        metrics['f1'].append(f1)
        metrics['mcc'].append(mcc)

    # フルデータで学習したモデルを保存 (Coefficients)
    model_full = LinearRegression()
    model_full.fit(X, y)
    feature_importances = model_full.coef_
    column_names = df.columns[:-1]
    feature_importances_df = pd.DataFrame({'Feature': column_names, 'Importance': feature_importances})
    feature_importances_df['Abs_Importance'] = feature_importances_df['Importance'].abs()
    feature_importances_df = feature_importances_df.sort_values(by='Abs_Importance', ascending=False).drop('Abs_Importance', axis=1)

    os.makedirs(path_out, exist_ok=True)
    excel_filename = os.path.join(path_out, 'feature_importances_linear.xlsx')
    feature_importances_df.to_excel(excel_filename, index=False)

    # 上位10特徴を可視化
    importance_sorted_idx = np.argsort(np.abs(feature_importances))[::-1]
    top_idxs = importance_sorted_idx[:10]

    # 結果表示・DataFrame生成
    res2 = make_result_df(metrics, 'Linear Regression')

    plt.figure(figsize=(10, 8))
    plt.barh(range(len(top_idxs)), feature_importances[top_idxs], align='center')
    plt.yticks(range(len(top_idxs)), [column_names[i] for i in top_idxs])
    plt.xlabel('Feature Importance')
    plt.title('Top 10 Feature Importances from Linear Regression Model')
    plt.gca().invert_yaxis()
    plt.show()

    # モデルを保存
    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_linear_model.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(model_full, file)

    return res2

##########################################
# Lasso Regression
##########################################
def LassoKFold(alpha_value=1.0, k=k):
    from sklearn.linear_model import Lasso

    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)
    lasso = Lasso(alpha=alpha_value)

    # 指標リスト
    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }

    feature_importance = np.zeros((X.shape[1],))

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        lasso.fit(X_train, y_train)
        predictions = lasso.predict(X_test)

        feature_importance += np.abs(lasso.coef_)

        predictions_round = [1 if p >= 0.5 else 0 for p in predictions]

        auc = roc_auc_score(y_test, predictions)
        acc = accuracy_score(y_test, predictions_round)
        prec = precision_score(y_test, predictions_round, zero_division=0)
        rec = recall_score(y_test, predictions_round)
        f1 = f1_score(y_test, predictions_round)
        mcc = compute_mcc(y_test, predictions_round)

        metrics['auc'].append(auc)
        metrics['acc'].append(acc)
        metrics['prec'].append(prec)
        metrics['rec'].append(rec)
        metrics['f1'].append(f1)
        metrics['mcc'].append(mcc)

    feature_importance /= k
    feature_names = df.columns[:-1]
    feature_importance_series = pd.Series(feature_importance, index=feature_names)
    sorted_importance = feature_importance_series.sort_values(ascending=False)[:10]

    # 結果表示・DataFrame
    res2 = make_result_df(metrics, f'Lasso Regression (α={alpha_value:.4f})')

    # グラフ
    plt.figure(figsize=(10, 8))
    plt.title('Top 10 Feature Importances from k-foldCV of Lasso Regression')
    sorted_importance.plot(kind='barh')
    plt.gca().invert_yaxis()
    plt.xlabel('Average Absolute Coefficient')
    plt.show()
    print('')

    # モデルをフルデータで再学習 & 保存
    model_full = Lasso(alpha=alpha_value)
    model_full.fit(X, y)
    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_lasso_model.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(model_full, file)

    return res2

##########################################
# simple Lasso Regression (Optuna use in other function)
##########################################
def simpleLassoKFold(alpha_value, k=k):
    from sklearn.linear_model import Lasso
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)
    lasso = Lasso(alpha=alpha_value)

    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        lasso.fit(X_train, y_train)
        predictions = lasso.predict(X_test)
        predictions_round = [1 if p >= 0.5 else 0 for p in predictions]

        auc = roc_auc_score(y_test, predictions)
        acc = accuracy_score(y_test, predictions_round)
        prec = precision_score(y_test, predictions_round, zero_division=0)
        rec = recall_score(y_test, predictions_round)
        f1 = f1_score(y_test, predictions_round)
        mcc = compute_mcc(y_test, predictions_round)

        metrics['auc'].append(auc)
        metrics['acc'].append(acc)
        metrics['prec'].append(prec)
        metrics['rec'].append(rec)
        metrics['f1'].append(f1)
        metrics['mcc'].append(mcc)

    # ここでは最終的な表示だけ（Optuna最適化用）
    res2 = make_result_df(metrics, f'Lasso Regression (α={alpha_value:.4f})')
    return res2

##########################################
# Lasso Optimized with Optuna
##########################################
import optuna
from sklearn.metrics import mean_squared_error

def objective_lasso(trial, X, y, k=k):
    from sklearn.linear_model import Lasso
    alpha = trial.suggest_float('alpha', 0.0001, 10.0, log=True)
    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)
    mse_scores = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        model = Lasso(alpha=alpha, random_state=random_state)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        mse_scores.append(mse)

    return np.mean(mse_scores)

def LassoKFoldOptuna(k=k):
    from sklearn.linear_model import Lasso
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    study = optuna.create_study(direction='minimize')
    optuna.logging.disable_default_handler()
    study.optimize(lambda trial: objective_lasso(trial, X, y, k), n_trials=n_trialsA)

    best_alpha = study.best_trial.params['alpha']

    print('')
    print('◆ Hyperparameter Optimization for Lasso with Optuna')
    print(f'Best trial for minimizing MSE:')
    print(f'  Value: {study.best_trial.value}')
    print(f'  Params: ')
    for key, value in study.best_trial.params.items():
        print(f'    {key}: {value}')
    print('')

    best_model = Lasso(alpha=best_alpha, random_state=random_state)
    best_model.fit(X, y)

    # Evaluate with classification metrics (k-fold)
    res = simpleLassoKFold(best_alpha, k=k)

    # Feature importance
    feature_names = df.columns[:-1]
    feature_importance = np.abs(best_model.coef_)
    feature_importance_series = pd.Series(feature_importance, index=feature_names)
    sorted_importance = feature_importance_series.sort_values(ascending=False)[:10]

    plt.figure(figsize=(10, 8))
    plt.title('Top 10 Feature Importances from Best Lasso Model Optimized with Optuna')
    sorted_importance.plot(kind='barh')
    plt.gca().invert_yaxis()
    plt.xlabel('Absolute Coefficient')
    plt.show()

    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_optuna_lasso_model.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(best_model, file)

    return res

##########################################
# Ridge Regression
##########################################
def RidgeKFold(alpha_value=1.0, k=k):
    from sklearn.linear_model import Ridge

    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)
    ridge = Ridge(alpha=alpha_value)

    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }

    feature_importances = np.zeros(X.shape[1])

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        ridge.fit(X_train, y_train)
        predictions = ridge.predict(X_test)

        feature_importances += np.abs(ridge.coef_)

        predictions_round = [1 if p >= 0.5 else 0 for p in predictions]

        auc = roc_auc_score(y_test, predictions)
        acc = accuracy_score(y_test, predictions_round)
        prec = precision_score(y_test, predictions_round, zero_division=0)
        rec = recall_score(y_test, predictions_round)
        f1 = f1_score(y_test, predictions_round)
        mcc = compute_mcc(y_test, predictions_round)

        metrics['auc'].append(auc)
        metrics['acc'].append(acc)
        metrics['prec'].append(prec)
        metrics['rec'].append(rec)
        metrics['f1'].append(f1)
        metrics['mcc'].append(mcc)

    # 平均的な特徴重要度(係数の絶対値)
    feature_importances /= k
    feature_importances = pd.Series(feature_importances, index=df.columns[:-1])
    top10_features = feature_importances.nlargest(10)

    # 結果表示・DataFrame
    res2 = make_result_df(metrics, f'Ridge Regression (α={alpha_value:.4f})')

    plt.figure(figsize=(10, 8))
    top10_features.plot(kind='barh').invert_yaxis()
    plt.title('Top 10 Feature Importances from k-foldCV of Ridge Regression')
    plt.xlabel('Mean Absolute Coefficient')
    plt.show()

    model_full = Ridge(alpha=alpha_value)
    model_full.fit(X, y)

    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_ridge_model.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(model_full, file)

    return res2

##########################################
# simple Ridge Regression (used in optuna)
##########################################
def simpleRidgeKFold(alpha_value, k=k):
    from sklearn.linear_model import Ridge
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)
    ridge = Ridge(alpha=alpha_value)

    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        ridge.fit(X_train, y_train)
        predictions = ridge.predict(X_test)

        predictions_round = [1 if p >= 0.5 else 0 for p in predictions]

        auc = roc_auc_score(y_test, predictions)
        acc = accuracy_score(y_test, predictions_round)
        prec = precision_score(y_test, predictions_round, zero_division=0)
        rec = recall_score(y_test, predictions_round)
        f1 = f1_score(y_test, predictions_round)
        mcc = compute_mcc(y_test, predictions_round)

        metrics['auc'].append(auc)
        metrics['acc'].append(acc)
        metrics['prec'].append(prec)
        metrics['rec'].append(rec)
        metrics['f1'].append(f1)
        metrics['mcc'].append(mcc)

    res2 = make_result_df(metrics, f'Ridge Regression (α={alpha_value:.4f})')
    return res2

##########################################
# Ridge Optimized with Optuna
##########################################
def objective_ridge(trial, X, y, k=k):
    from sklearn.linear_model import Ridge
    alpha = trial.suggest_float('alpha', 0.0001, 10.0, log=True)

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)
    mse_scores = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model = Ridge(alpha=alpha, random_state=random_state)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        mse_scores.append(mse)

    return np.mean(mse_scores)

def RidgeKFoldOptuna(k=k):
    from sklearn.linear_model import Ridge
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    study = optuna.create_study(direction='minimize')
    optuna.logging.disable_default_handler()
    study.optimize(lambda trial: objective_ridge(trial, X, y, k), n_trials=n_trialsA)

    best_alpha = study.best_trial.params['alpha']

    print('')
    print('◆ Hypterparameter Optimization for Ridge with Optuna')
    print(f'Best trial for minimizing MSE:')
    print(f'  Value: {study.best_trial.value}')
    print(f'  Params: ')
    for key, value in study.best_trial.params.items():
        print(f'    {key}: {value}')
    print('')

    best_model = Ridge(alpha=best_alpha, random_state=random_state)
    best_model.fit(X, y)

    res = simpleRidgeKFold(best_alpha, k=k)

    feature_names = df.columns[:-1]
    feature_importance = np.abs(best_model.coef_)
    feature_importance_series = pd.Series(feature_importance, index=feature_names)
    sorted_importance = feature_importance_series.sort_values(ascending=False)[:10]

    plt.figure(figsize=(10, 8))
    plt.title('Top 10 Feature Importances from Best Ridge Model optimized with Optuna')
    sorted_importance.plot(kind='barh')
    plt.gca().invert_yaxis()
    plt.xlabel('Absolute Coefficient')
    plt.show()
    print('')

    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_optuna_ridge_model.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(best_model, file)

    return res

##########################################
# Logistic Normalized
##########################################
def LogisticKFoldNormalized(k=k):
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import MinMaxScaler

    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)

    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }

    feature_importances = np.zeros((df.shape[1] - 1,))

    for train_index, test_index in kf.split(X_scaled):
        X_train, X_test = X_scaled[train_index], X_scaled[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model = LogisticRegression(random_state=random_state)
        model.fit(X_train, y_train)

        feature_importances += np.abs(model.coef_[0])

        y_pred = model.predict(X_test)
        y_pred_prob = model.predict_proba(X_test)[:, 1]

        auc = roc_auc_score(y_test, y_pred_prob)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        mcc = compute_mcc(y_test, y_pred)

        metrics['auc'].append(auc)
        metrics['acc'].append(accuracy)
        metrics['prec'].append(precision)
        metrics['rec'].append(recall)
        metrics['f1'].append(f1)
        metrics['mcc'].append(mcc)

    # 結果
    res2 = make_result_df(metrics, 'Logistic Regression Normalized')

    feature_importances /= k
    feature_names = df.columns[:-1]
    sorted_idx = np.argsort(feature_importances)[-10:]
    sorted_importance = feature_importances[sorted_idx]
    sorted_features = feature_names[sorted_idx]

    plt.figure(figsize=(10, 8))
    plt.barh(sorted_features, sorted_importance, color='skyblue')
    plt.xlabel('Average Feature Importance')
    plt.title('Top 10 Feature Importances from k-foldCV of Logistic Regression normalized')
    plt.show()

    model_full = LogisticRegression(random_state=random_state)
    model_full.fit(X_scaled, y)
    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_logistic_normalized.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(model_full, file)

    return res2

##########################################
# Logistic Regression with Standardization
##########################################
def LogisticKFoldStandardized(k=k):
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler

    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    scaler = StandardScaler()
    X_standardized = scaler.fit_transform(X)

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)

    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }
    feature_importances = np.zeros((df.shape[1] - 1,))

    for train_index, test_index in kf.split(X_standardized):
        X_train, X_test = X_standardized[train_index], X_standardized[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model = LogisticRegression(random_state=random_state)
        model.fit(X_train, y_train)

        feature_importances += np.abs(model.coef_[0])

        y_pred = model.predict(X_test)
        y_pred_prob = model.predict_proba(X_test)[:, 1]

        auc = roc_auc_score(y_test, y_pred_prob)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        mcc = compute_mcc(y_test, y_pred)

        metrics['auc'].append(auc)
        metrics['acc'].append(accuracy)
        metrics['prec'].append(precision)
        metrics['rec'].append(recall)
        metrics['f1'].append(f1)
        metrics['mcc'].append(mcc)

    res2 = make_result_df(metrics, 'Logistic Regression Standardized')

    feature_importances /= k
    feature_names = df.columns[:-1]
    sorted_idx = np.argsort(feature_importances)[-10:]
    sorted_importance = feature_importances[sorted_idx]
    sorted_features = feature_names[sorted_idx]

    plt.figure(figsize=(10, 8))
    plt.barh(sorted_features, sorted_importance, color='skyblue')
    plt.xlabel('Average Feature Importance')
    plt.title('Top 10 Feature Importances from k-foldCV of Logistic Regression standardized')
    plt.show()

    model_full = LogisticRegression(random_state=random_state)
    model_full.fit(X_standardized, y)

    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_logistic_standardized.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(model_full, file)

    return res2

##########################################
# SVM Normalized
##########################################
def SVMKFoldNormalized(k=k):
    from sklearn.svm import SVC
    from sklearn.preprocessing import MinMaxScaler

    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)

    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }
    feature_importances = np.zeros((df.shape[1] - 1,))

    for train_index, test_index in kf.split(X_scaled):
        X_train, X_test = X_scaled[train_index], X_scaled[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model = SVC(kernel='linear', probability=True, random_state=random_state)
        model.fit(X_train, y_train)

        if model.kernel == 'linear':
            feature_importances += np.abs(model.coef_[0])

        y_pred = model.predict(X_test)
        y_pred_prob = model.predict_proba(X_test)[:, 1]

        auc = roc_auc_score(y_test, y_pred_prob)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        mcc = compute_mcc(y_test, y_pred)

        metrics['auc'].append(auc)
        metrics['acc'].append(accuracy)
        metrics['prec'].append(precision)
        metrics['rec'].append(recall)
        metrics['f1'].append(f1)
        metrics['mcc'].append(mcc)

    res2 = make_result_df(metrics, 'SVM Normalized')

    feature_importances /= k
    feature_names = df.columns[:-1]
    sorted_idx = np.argsort(feature_importances)[-10:]
    sorted_importance = feature_importances[sorted_idx]
    sorted_features = feature_names[sorted_idx]

    plt.figure(figsize=(10, 8))
    plt.barh(sorted_features, sorted_importance, color='skyblue')
    plt.xlabel('Average Feature Importance')
    plt.title('Top 10 Feature Importances from k-foldCV of linear SVM normalized')
    plt.show()
    print('')

    model_full = SVC(probability=True, random_state=random_state)
    model_full.fit(X_scaled, y)

    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_svm_normalized.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(model_full, file)

    return res2

##########################################
# SVM Standardized
##########################################
def SVMKFoldStandardized(k=k):
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler

    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)

    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }
    feature_importances = np.zeros((X.shape[1],))

    for train_index, test_index in kf.split(X_scaled):
        X_train, X_test = X_scaled[train_index], X_scaled[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model = SVC(kernel='linear', probability=True, random_state=random_state)
        model.fit(X_train, y_train)

        if model.kernel == 'linear':
            feature_importances += np.abs(model.coef_[0])

        y_pred = model.predict(X_test)
        y_pred_prob = model.predict_proba(X_test)[:, 1]

        auc = roc_auc_score(y_test, y_pred_prob)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        mcc = compute_mcc(y_test, y_pred)

        metrics['auc'].append(auc)
        metrics['acc'].append(accuracy)
        metrics['prec'].append(precision)
        metrics['rec'].append(recall)
        metrics['f1'].append(f1)
        metrics['mcc'].append(mcc)

    res2 = make_result_df(metrics, 'SVM Standardized')

    feature_importances /= k
    feature_names = df.columns[:-1]
    sorted_idx = np.argsort(feature_importances)[-10:]
    sorted_importance = feature_importances[sorted_idx]
    sorted_features = feature_names[sorted_idx]

    plt.figure(figsize=(10, 8))
    plt.barh(sorted_features, sorted_importance, color='skyblue')
    plt.xlabel('Average Feature Importance')
    plt.title('Top 10 Feature Importances from k-foldCV of linear SVM standardized')
    plt.show()
    print('')

    model_full = SVC(probability=True, random_state=random_state)
    model_full.fit(X_scaled, y)

    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_svm_standardized.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(model_full, file)

    return res2

##########################################
# RandomForest
##########################################
def RandomForestKFold(k=k):
    from sklearn.ensemble import RandomForestClassifier

    X = df.iloc[:, :-1]
    y = df.iloc[:, -1].values

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)

    metrics = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }
    feature_importance_list = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model = RandomForestClassifier(random_state=random_state)
        model.fit(X_train, y_train)

        feature_importances = model.feature_importances_
        feature_importance_list.append(feature_importances)

        y_pred = model.predict(X_test)
        y_pred_prob = model.predict_proba(X_test)[:, 1]

        auc_ = roc_auc_score(y_test, y_pred_prob)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred)
        f1_ = f1_score(y_test, y_pred)
        mcc = compute_mcc(y_test, y_pred)

        metrics['auc'].append(auc_)
        metrics['acc'].append(accuracy)
        metrics['prec'].append(precision)
        metrics['rec'].append(recall)
        metrics['f1'].append(f1_)
        metrics['mcc'].append(mcc)

    mean_feature_importances = np.mean(feature_importance_list, axis=0)
    indices = np.argsort(mean_feature_importances)[-10:]
    top_features = X.columns[indices]
    top_importances = mean_feature_importances[indices]

    # 結果表示・DataFrame
    res2 = make_result_df(metrics, 'RandomForest')

    plt.figure(figsize=(10, 6))
    plt.barh(range(len(indices)), top_importances, color='b', align='center')
    plt.yticks(range(len(indices)), [X.columns[i] for i in indices])
    plt.xlabel('Relative Importance')
    plt.title('Top 10 Feature Importances from Random Forest Model')
    plt.gca().invert_yaxis()
    plt.show()

    model_full = RandomForestClassifier(random_state=random_state)
    model_full.fit(X, y)

    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_random_forest.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(model_full, file)

    return res2

##########################################
# RandomForest Optuna
##########################################
def RandomForestOptunaKFold(k=k):
    from sklearn.ensemble import RandomForestClassifier

    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    def objective_rf(trial):
        n_estimators = trial.suggest_int('n_estimators', 10, 500)
        max_depth = trial.suggest_int('max_depth', 2, 32, log=True)
        min_samples_split = trial.suggest_int('min_samples_split', 2, 14)
        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 14)
        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])

        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features,
            random_state=random_state,
        )

        kf_ = KFold(n_splits=k, shuffle=True, random_state=random_state)
        auc_scores_ = cross_val_score(model, X, y, cv=kf_, scoring='roc_auc')
        return np.mean(auc_scores_)

    study = optuna.create_study(direction='maximize')
    optuna.logging.disable_default_handler()
    study.optimize(objective_rf, n_trials=n_trialsB)

    print('◆ Hyperparameter Optimization for Random Forest with Optuna')
    print("Best hyperparameters: ", study.best_params)

    best_model = RandomForestClassifier(
        n_estimators=study.best_params['n_estimators'],
        max_depth=study.best_params['max_depth'],
        min_samples_split=study.best_params['min_samples_split'],
        min_samples_leaf=study.best_params['min_samples_leaf'],
        max_features=study.best_params['max_features'],
        random_state=random_state,
    )

    # CV で各指標を計算
    kf_ = KFold(n_splits=k, shuffle=True, random_state=random_state)

    # cross_validate を使っても良いが、MCCは自前で計算
    scoring_metrics = ['roc_auc', 'accuracy', 'precision', 'recall', 'f1']

    final_scores = {
        'auc': [],
        'acc': [],
        'prec': [],
        'rec': [],
        'f1': [],
        'mcc': []
    }

    for train_idx, test_idx in kf_.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        best_model.fit(X_train, y_train)
        y_pred = best_model.predict(X_test)
        y_pred_prob = best_model.predict_proba(X_test)[:, 1]

        final_scores['auc'].append(roc_auc_score(y_test, y_pred_prob))
        final_scores['acc'].append(accuracy_score(y_test, y_pred))
        final_scores['prec'].append(precision_score(y_test, y_pred, zero_division=0))
        final_scores['rec'].append(recall_score(y_test, y_pred))
        final_scores['f1'].append(f1_score(y_test, y_pred))
        final_scores['mcc'].append(matthews_corrcoef(y_test, y_pred))

    print('')
    print(f'■ {k}-fold Cross Validation と Optuna による最適化を行った RandomForest による2値分類の検定結果')
    print('')

    # DataFrame へまとめて表示
    res2 = make_result_df(final_scores, 'RandomForest(Optuna)')

    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_optuna_random_forest.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(best_model, file)

    return res2

##########################################
# XGBoost (default)
##########################################

def XGBoostKFold(k=k):
    from xgboost import XGBClassifier

    # 読み込み済みの df を利用する想定
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values
    feature_names = df.columns[:-1]

    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)

    auc_scores, accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores = [], [], [], [], [], []
    feature_importances = defaultdict(list)

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Initialize the XGBoost model with default parameters
        model = XGBClassifier(
            booster='gblinear',
            use_label_encoder=False,
            eval_metric='logloss',
            random_state=random_state
        )
        model.fit(X_train, y_train)

        # Predict classes and probabilities for evaluation
        y_pred = model.predict(X_test)
        y_pred_prob = model.predict_proba(X_test)[:, 1]

        # Compute metrics for the current fold
        auc = roc_auc_score(y_test, y_pred_prob)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        mcc = matthews_corrcoef(y_test, y_pred)

        # Append scores
        auc_scores.append(auc)
        accuracy_scores.append(accuracy)
        precision_scores.append(precision)
        recall_scores.append(recall)
        f1_scores.append(f1)
        mcc_scores.append(mcc)

        # Getting feature importances
        for i, importance in enumerate(model.feature_importances_):
            feature_importances[feature_names[i]].append(importance)

    # Calculate and print the mean ± std of each metric
    print(f'■ {k}-fold Cross Validation を用いた DEFAULT の hyperparameters での XGBoost による2値分類の検定結果')
    print('')
    print(f'Mean AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}')
    print(f'Mean Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}')
    print(f'Mean Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}')
    print(f'Mean Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}')
    print(f'Mean F1 Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}')
    print(f'Mean MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}')
    print('')

    # Average Importance
    avg_importance = {feature: np.mean(importances) for feature, importances in feature_importances.items()}
    sorted_avg_importance = dict(sorted(avg_importance.items(), key=lambda item: item[1], reverse=True))

    # Visualization Functions
    def plot_importance(title, importance_dict, color, xlabel):
        plt.figure(figsize=(10, 8))
        plt.title(title)
        keys = list(importance_dict.keys())[:10]
        values = list(importance_dict.values())[:10]
        plt.barh(keys, values, color=color)
        plt.xlabel(xlabel)
        plt.gca().invert_yaxis()
        plt.show()

    print('')
    print(f'◆ {k}-fold Cross Validation を用いた XGBoost による Feature Importance')
    print('')

    # Plot for Average Importance
    plot_importance("Top 10 Feature Importances (Average) from XGBoost",
                    dict(list(sorted_avg_importance.items())[:10]),
                    'skyblue',
                    "Average Importance")

    # Retrain the model on the full dataset
    model_full = XGBClassifier(
        booster='gblinear',
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=random_state
    )
    model_full.fit(X, y)

    # Ensure the directory exists
    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_xgboost.pkl')
    # Save the best model
    with open(model_filename, 'wb') as file:
        pickle.dump(model_full, file)

    # Return DataFrame with mean values
    res = np.array([
        np.mean(auc_scores),
        np.mean(accuracy_scores),
        np.mean(precision_scores),
        np.mean(recall_scores),
        np.mean(f1_scores),
        np.mean(mcc_scores)
    ], dtype=object)

    res2 = pd.DataFrame([res],
                        columns=['AUC', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'MCC'],
                        index=['XGBoost'])
    return res2


def XGBoostOptunaKFold(k=k):
    import xgboost as xgb
    import optuna

    # Assuming df is your DataFrame loaded previously
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values
    feature_names = df.columns[:-1]

    def objective(trial):
        # Define the hyperparameter space
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'max_depth': trial.suggest_int('max_depth', 3, 9),
            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        }

        # Initialize and train the model
        model = xgb.XGBClassifier(
            **params,
            use_label_encoder=False,
            eval_metric='logloss',
            random_state=random_state,
            booster='gblinear'
        )

        # KFold cross-validation
        kf = KFold(n_splits=k, shuffle=True, random_state=random_state)
        auc_scores = []

        for train_index, test_index in kf.split(X):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            model.fit(
                X_train, y_train,
#                eval_set=[(X_test, y_test)],
#                early_stopping_rounds=50,
#                verbose=False
            )

            y_pred_prob = model.predict_proba(X_test)[:, 1]
            auc = roc_auc_score(y_test, y_pred_prob)
            auc_scores.append(auc)

        return np.mean(auc_scores)

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trialsC)  # Adjust n_trials according to your computational resources

    print('')
    print("◆ Best hyperparameters: ", study.best_params)
    print('')

    # Final KFold cross-validation with the best model
    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)
    final_scores = {
        'roc_auc': [],
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'mcc': []
    }

    feature_importances = defaultdict(list)

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        best_model = xgb.XGBClassifier(
            **study.best_params,
            use_label_encoder=False,
            eval_metric='logloss',
            random_state=random_state
        )
        best_model.fit(X, y)

        # Predict classes and probabilities for evaluation
        y_pred = best_model.predict(X_test)
        y_pred_prob = best_model.predict_proba(X_test)[:, 1]

        # Compute metrics for the current fold
        final_scores['roc_auc'].append(roc_auc_score(y_test, y_pred_prob))
        final_scores['accuracy'].append(accuracy_score(y_test, y_pred))
        final_scores['precision'].append(precision_score(y_test, y_pred, zero_division=0))
        final_scores['recall'].append(recall_score(y_test, y_pred))
        final_scores['f1'].append(f1_score(y_test, y_pred))
        final_scores['mcc'].append(matthews_corrcoef(y_test, y_pred))

        # Getting feature importances
        for i, importance in enumerate(best_model.feature_importances_):
            feature_importances[feature_names[i]].append(importance)

    print('')
    print(f'■ {k}-fold Cross Validation を用いた XGBoost optimized with Optuna による2値分類の検定結果')
    print('')
    for metric, scores in final_scores.items():
        # metric の名前に合わせて見やすく capitalize など実行
        print(f"{metric.upper()}: Mean={np.mean(scores):.4f} ± {np.std(scores):.4f}")
    print('')

    print(f'◆ {k}-fold Cross Validation と Optuna で得られた XGBoost の Best Model における Feature Importance')
    print('')

    # Average Importance
    avg_importance = {feature: np.mean(importances) for feature, importances in feature_importances.items()}
    sorted_avg_importance = dict(sorted(avg_importance.items(), key=lambda item: item[1], reverse=True))

    # Visualization Functions
    def plot_importance(title, importance_dict, color, xlabel):
        plt.figure(figsize=(10, 8))
        plt.title(title)
        keys = list(importance_dict.keys())[:10]
        values = list(importance_dict.values())[:10]
        plt.barh(keys, values, color=color)
        plt.xlabel(xlabel)
        plt.gca().invert_yaxis()
        plt.show()

    # Plot for Average Importance
    plot_importance("Top 10 Feature Importances (Average) from XGBoost",
                    dict(list(sorted_avg_importance.items())[:10]),
                    'skyblue',
                    "Average Importance")

    # Train best model with full data
    best_model_full = xgb.XGBClassifier(
        **study.best_params,
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=random_state
    )
    best_model_full.fit(X, y)

    # Ensure the directory exists
    os.makedirs(path_model, exist_ok=True)
    model_filename = os.path.join(path_model, 'kFCV_xgboost_optuna.pkl')
    with open(model_filename, 'wb') as file:
        pickle.dump(best_model_full, file)

    # Prepare the final result (mean of each metric)
    # final_scores のキーの順番に対応する列名を定義
    final_cols = ['roc_auc', 'accuracy', 'precision', 'recall', 'f1', 'mcc']
    col_names = ['AUC', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'MCC']

    res = [np.mean(final_scores[c]) for c in final_cols]
    res2 = pd.DataFrame([res], columns=col_names, index=['XGBoostOptuna'])

    return res2

##########################################
# 実行パート
##########################################

print('')
res_linear = LinearKFold(k=k)
print('')
res_lasso = LassoKFold(alpha_value=0.2095426564930675, k=k)
print('')
res_ridge = RidgeKFoldOptuna(k=k)
print('')
res_logistic_N = LogisticKFoldNormalized(k=k)
print('')
res_logistic_S = LogisticKFoldStandardized(k=k)
print('')
res_SVM_N = SVMKFoldNormalized(k=k)
print('')
res_SVM_S = SVMKFoldStandardized(k=k)
print('')
res_RandomForest = RandomForestKFold(k=k)
# もしOptuna版を使うなら↓コメントアウト解除:
res_RandomForest = RandomForestOptunaKFold(k=k)
print('')
# もしdefault XGBoostを比較したいなら↓コメントアウト解除:
# res_XGBoost_default = XGBoostKFold(k=k)
# print('')

res_XGBoost = XGBoostOptunaKFold(k=k)
print('')

# 各結果をまとめる
results = pd.concat([
    res_linear,
    res_lasso,
    res_ridge,
    res_logistic_N,
    res_logistic_S,
    res_SVM_N,
    res_SVM_S,
    res_RandomForest,
    res_XGBoost
], axis=0)

# まとめて出力
results.to_excel(filepath3)